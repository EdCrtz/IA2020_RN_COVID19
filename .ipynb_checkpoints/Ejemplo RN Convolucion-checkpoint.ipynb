{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo Red Neuronal Covoluvional\n",
    "\n",
    "Para ello se utilizarán la libreria **keras**, la cual nos permite crear redes neuronales con componentes, sin que nos tengamos que preocupar mucho por las dimensiones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D , Convolution2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(train_samples, train_labels), (test_samples, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última línea del código (arriba) importa _MNLIST_ del repositorio de **keras**, en ella se cargan ejemplos de entrenamiento, etiquetas de entrenamiento, ejemplos de prueba y etiquetas de prueba en cuatro variables diferentes.\n",
    "\n",
    "La mayor parte del código en este archivo _Python_ se usará para formatear (o preprocesar) datos MNIST para cumplir con las caracteristicas que deben tener los datos para procesar en la red neuronal convolucional. La siguiente parte del código procesa las imágenes _MNIST_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reshape(train_samples.shape [0], 28, 28, 1)\n",
    "test_samples = test_samples.reshape(test_samples.shape [0], 28, 28, 1)\n",
    "train_samples = train_samples.astype(\"float32\")\n",
    "test_samples = test_samples.astype(\"float32\")\n",
    "train_samples = train_samples/255\n",
    "test_samples = test_samples/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero observamos que el código está duplicado: las operaciones se realizan tanto en el conjunto de entrenamiento como en el conjunto de prueba, por lo tanto se expliacara solo un conjunto, para el otro se hace lo mismo. \n",
    "\n",
    "La primera línea de este bloque de código da nueva forma a la matriz que contiene _MNIST_ . El resultado de esta remodelación es una matriz _(60000, 28, 28, 1)_ . La primera dimensión es simplemente el número de muestras, la segunda y la tercera están aquí para representar la dimensión de 28 por 28 imágenes, y el último es el canal. \n",
    "\n",
    "Podría ser RGB, pero _MNIST_ está en escala de grises, por lo que esto puede parecer redundante, pero el objetivo de remodelar la matriz (la dimensión inicial era _(60000, 28, 28)_ ) era en realidad agregar la dimensión final con 1 componente.\n",
    "\n",
    "La razón detrás de esto es que a medida que avanzamos a través de capas convolucionales, se agregarán mapas de características en esta dirección, por lo que debemos preparar el tensor para poder aceptarlo. \n",
    "\n",
    "La tercera fila declara que las entradas en la matriz son de tipo _float32_ . Esto simplemente significa que deben tratarse como números decimales. Python lo haría automáticamente, pero Numpy, que acelera el cálculo drásticamente, necesita declaraciones de tipo, por lo que tenemos que poner esta línea. \n",
    "\n",
    "La quinta línea normaliza las entradas de matriz de un rango de 0 a 255 a un rango entre 0 y 1 (a ser interpretado como el porcentaje de gris en un píxel). \n",
    "\n",
    "Ahora debemos preprocesar las etiquetas (dígitos del 0 al 9) con una codificación activa. Hacemos esto con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "c_test_labels = np_utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con eso hemos terminado de preprocesar los datos y podemos continuar construyendo la red neuronal convolucional real. El siguiente código especifica las capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cance\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  \n",
      "C:\\Users\\cance\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "convnet = Sequential()\n",
    "convnet.add(Convolution2D(32, 4, 4, activation='relu', input_shape=(28,28,1)))\n",
    "convnet.add(MaxPooling2D(pool_size=(2,2)))\n",
    "convnet.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "convnet.add(MaxPooling2D(pool_size=(2,2)))\n",
    "convnet.add(Dropout(0.3))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera línea de este bloque de código crea un nuevo modelo en blanco, y el resto de las líneas definen la especificación de red.\n",
    "\n",
    "La segunda línea agrega la primera capa, en este caso es una capa convolucional, que tiene que producir 32 mapas de características, tiene _ReLU_ como la función de activación y tiene un campo receptivo de 4 por 4.\n",
    "\n",
    "Para la primera capa, también tenemos que especificar las dimensiones de entrada para cada muestra de entrenamiento que le proporcionaremos. **Keras** toma la primera dimensión de una matriz para representar muestras de entrenamiento individuales y corta (analiza) el conjunto de datos a lo largo de él, por lo que no debemos preocuparnos por dar un tensor _(65600, 28, 28, 1)_ en lugar de un _(60000, 28, 28, 1)_ después de que hayamos especificado que toma _input_shape = (28, 28, 1)_ , pero el código se bloqueará si le damos un _(60000, 29, 29, 1)_ o incluso un _(60000, 28, 28)_ conjunto de datos.\n",
    "\n",
    "La tercera fila define una capa de agrupación máxima con un tamaño de agrupación de 2 por 2. La siguiente línea especifica una tercera capa, que es una capa convolucional, esta vez con un campo receptivo de 3 por 3. Aquí no tenemos que especificar el dimensiones de entrada, **Keras** lo hará por nosotros.\n",
    "\n",
    "A continuación, tenemos otra capa de agrupación máxima, también con un tamaño de agrupación de 2 por 2. Después de esto, tenemos una \"capa\" de abandono. Esta no es una capa real, sino solo una modificación de las conexiones entre la capa anterior y la siguiente. Las conexiones se modifican para incluir una tasa de abandono de 0.3 para todas las conexiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente linea aplana el tensor. Esta es una versión generalizada del proceso que describimos para traducir matrices de tamaño fijo en un vector, solo aquí se generaliza para tensores arbitrarios.\n",
    "\n",
    "El vector aplanado se alimenta a la capa final (la línea final de código en este bloque), que es una capa de alimentación directa totalmente conectada estándar, que acepta tanto entradas ya que hay componentes en el vector aplanado y emitiendo 10 valores (10 neuronas de salida), donde cada una de ellas representará un dígito y generará la probabilidad respectiva. \n",
    "\n",
    "¿Cuál de ellos representa qué dígito está realmente definido solo por el orden que teníamos cuando hicimos una codificación en caliente de las etiquetas?\n",
    "\n",
    "La función de activación _softmax_ utilizada en la capa final es una versión de la logística funcionan para más de dos clases, pero lo describiremos en los capítulos posteriores, para ahora solo piense en ello como una función logística para muchas clases (tenemos una clase para cada etiqueta 0–9). Ahora tenemos un modelo especificado y debemos compilarlo. \n",
    "\n",
    "Compilando un modelo significa que **Keras** ahora puede deducir y completar todos los detalles necesarios que no especificó, como el tamaño de entrada para la segunda capa convolucional, o el dimensionalidad del vector aplanado. La siguiente línea de código compila el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos ver que hemos especificado el método de entrenamiento para ser _\"sgd\"_ , que es el descenso de gradiente estocástico, con _MSE_ como la función de error. También hemos pedido **Keras** para calcular la precisión al entrenar. \n",
    "\n",
    "La siguiente línea de código entrena al modelo compilado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0900 - accuracy: 0.1280\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.0889 - accuracy: 0.2373\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0876 - accuracy: 0.3378\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0848 - accuracy: 0.3961\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0761 - accuracy: 0.4559\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0581 - accuracy: 0.6003\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0427 - accuracy: 0.7075\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0340 - accuracy: 0.7714\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0284 - accuracy: 0.8112\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0247 - accuracy: 0.8371\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.0219 - accuracy: 0.8573\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0199 - accuracy: 0.8710\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 0.0182 - accuracy: 0.8828\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 0.0170 - accuracy: 0.8903\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0159 - accuracy: 0.8973\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0150 - accuracy: 0.9028\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0142 - accuracy: 0.9077\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0135 - accuracy: 0.9129\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0128 - accuracy: 0.9181\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0124 - accuracy: 0.9204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f32e62d948>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.fit(train_samples, c_train_labels, batch_size=32, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta línea de código entrena el modelo usando _train_samples_ como ejemplos de entrenamiento y _c_train_labels_ como etiquetas de entrenamiento. También utiliza un tamaño de lote de 32 y trenes por 20 épocas El indicador _\"detallado\"_ se establece en 1, lo que significa que imprimirá detalles de formación. \n",
    "\n",
    "Y ahora continuamos a la parte final del código que imprime la precisión y hace predicciones de lo que ha aprendido para un nuevo conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = convnet.evaluate(test_samples, c_test_labels, verbose=1)\n",
    "print()\n",
    "print(\"%s: %.2f%%\" % (convnet.metrics_names[1], metrics[1]*100))\n",
    "predictions = convnet.predict(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última línea es importante. Aquí hemos puesto _test_samples_, pero si desea usarlo para predicciones, debe poner algunas muestras frescas aquí, teniendo en cuenta que tienen que tener exactamente las mismas dimensiones que _test_samples_ aparte de la primera dimensión, que contiene muestras de entrenamiento individuales y que **Keras** analiza el conjunto de datos. \n",
    "\n",
    "Las predicciones variables tendrán exactamente la misma dimensionalidad que _c_test_labels_ además de la primera dimensión, pero la primera dimensión de _test_samples_ y _c_test_labels_ será la igual (ya que son etiquetas predichas para ese conjunto de muestras). \n",
    "\n",
    "Puedes agregar un línea hasta el final diciendo _print(predictions)_ para ver las predicciones reales, o _print(predictions.shape)_ para ver la dimensionalidad de la matriz almacenada en predicciones.\n",
    "\n",
    "Estas 29 líneas de código (o 30 si agregó una de las últimas) formar una red convolucional completamente funcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicciones: \\n', predictions)\n",
    "print('\\nDimencionalidad \\n', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
